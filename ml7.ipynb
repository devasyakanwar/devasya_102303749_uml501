{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WbBvFT6yOdi",
        "outputId": "7fe06f8c-e641-4398-f4bd-eadfda5fbddf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                               text\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "(5572, 2)\n",
            "label\n",
            "0    4825\n",
            "1     747\n",
            "Name: count, dtype: int64\n",
            "(4457, 3000)\n",
            "(1115, 3000)\n",
            "0.8837783262284048\n",
            "0.8923766816143498\n",
            "[[930  36]\n",
            " [ 84  65]]\n",
            "0.11622167377159526\n",
            "1.0143534603557114\n",
            "0.26087922870101093\n",
            "0.5207018789750049\n",
            "0.3512557431516268\n",
            "0.3067620053374075\n",
            "0.36581609850424546\n",
            "0.275104116902884\n",
            "0.4429219882660905\n",
            "0.11465581563866874\n",
            "0.41177269499634184\n",
            "0.17832098020348391\n",
            "0.4286849954067594\n",
            "0.14360917827958558\n",
            "0.423971825276512\n",
            "0.15324478537279745\n",
            "0.43552512780910013\n",
            "0.129671687911075\n",
            "0.4381784558754228\n",
            "0.12427900070986571\n",
            "0.44595125533486046\n",
            "0.10852150828043412\n",
            "0.44044492057667584\n",
            "0.119678283888183\n",
            "0.44752638756957897\n",
            "0.10533508466602044\n",
            "0.44676589583366594\n",
            "0.10687325616274146\n",
            "0.4518882659570016\n",
            "0.09652210517008078\n",
            "0.9183307157280682\n",
            "0.9291479820627803\n",
            "[[959   7]\n",
            " [ 72  77]]\n",
            "0.9149652232443347\n",
            "0.9237668161434978\n",
            "[[966   0]\n",
            " [ 85  64]]\n",
            "[0.8837783262284048, 0.8923766816143498]\n",
            "[0.9183307157280682, 0.9291479820627803]\n",
            "[0.9149652232443347, 0.9237668161434978]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['label', 'text']\n",
        "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=3000)\n",
        "X = tfidf.fit_transform(df['cleaned_text']).toarray()\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = stump.predict(X_train)\n",
        "y_test_pred = stump.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(train_acc)\n",
        "print(test_acc)\n",
        "print(cm)\n",
        "\n",
        "class ManualAdaBoost:\n",
        "    def __init__(self, T=15):\n",
        "        self.T = T\n",
        "        self.alphas = []\n",
        "        self.stumps = []\n",
        "        self.errors = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        w = np.ones(n_samples) / n_samples\n",
        "        y_signed = 2 * y - 1\n",
        "\n",
        "        for t in range(self.T):\n",
        "            stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "            stump.fit(X, y, sample_weight=w)\n",
        "            predictions = stump.predict(X)\n",
        "            pred_signed = 2 * predictions - 1\n",
        "\n",
        "            misclassified = (predictions != y)\n",
        "            error = np.sum(w[misclassified])\n",
        "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
        "\n",
        "            alpha = 0.5 * np.log((1 - error) / error)\n",
        "\n",
        "            w = w * np.exp(-alpha * y_signed * pred_signed)\n",
        "            w = w / np.sum(w)\n",
        "\n",
        "            print(error)\n",
        "            print(alpha)\n",
        "\n",
        "            self.alphas.append(alpha)\n",
        "            self.stumps.append(stump)\n",
        "            self.errors.append(error)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        stump_preds = np.array([2*stump.predict(X) - 1 for stump in self.stumps])\n",
        "        weighted_sum = np.dot(self.alphas, stump_preds)\n",
        "        return (weighted_sum > 0).astype(int)\n",
        "\n",
        "manual_ada = ManualAdaBoost(T=15)\n",
        "manual_ada.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_manual = manual_ada.predict(X_train)\n",
        "y_test_pred_manual = manual_ada.predict(X_test)\n",
        "\n",
        "train_acc_manual = accuracy_score(y_train, y_train_pred_manual)\n",
        "test_acc_manual = accuracy_score(y_test, y_test_pred_manual)\n",
        "cm_manual = confusion_matrix(y_test, y_test_pred_manual)\n",
        "\n",
        "print(train_acc_manual)\n",
        "print(test_acc_manual)\n",
        "print(cm_manual)\n",
        "\n",
        "sklearn_ada = AdaBoostClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "sklearn_ada.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_sklearn = sklearn_ada.predict(X_train)\n",
        "y_test_pred_sklearn = sklearn_ada.predict(X_test)\n",
        "\n",
        "train_acc_sklearn = accuracy_score(y_train, y_train_pred_sklearn)\n",
        "test_acc_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
        "cm_sklearn = confusion_matrix(y_test, y_test_pred_sklearn)\n",
        "\n",
        "print(train_acc_sklearn)\n",
        "print(test_acc_sklearn)\n",
        "print(cm_sklearn)\n",
        "\n",
        "print([train_acc, test_acc])\n",
        "print([train_acc_manual, test_acc_manual])\n",
        "print([train_acc_sklearn, test_acc_sklearn])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv('heart.csv')\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "print(df.isnull().sum().sum())\n",
        "\n",
        "df = df.dropna()\n",
        "X = df.drop('target', axis=1)\n",
        "y = (df['target'].astype(int) > 0).astype(int)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "stump.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = stump.predict(X_train)\n",
        "y_test_pred = stump.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(train_acc)\n",
        "print(test_acc)\n",
        "print(cm)\n",
        "print(classification_report(y_test, y_test_pred, target_names=['No Disease','Disease']))\n",
        "\n",
        "\n",
        "n_estimators_list = [5, 10, 25, 50, 100]\n",
        "learning_rates = [0.1, 0.5, 1.0]\n",
        "\n",
        "results = {}\n",
        "best_acc = -1.0\n",
        "best_cfg = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    accs = []\n",
        "    for n_est in n_estimators_list:\n",
        "        ada = AdaBoostClassifier(\n",
        "            estimator=DecisionTreeClassifier(max_depth=1),\n",
        "            n_estimators=n_est,\n",
        "            learning_rate=lr,\n",
        "            random_state=42,\n",
        "            algorithm='SAMME'\n",
        "        )\n",
        "        ada.fit(X_train, y_train)\n",
        "        acc = accuracy_score(y_test, ada.predict(X_test))\n",
        "        accs.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            best_cfg = {'learning_rate': lr, 'n_estimators': n_est, 'model': ada}\n",
        "    results[lr] = accs\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(results[lr])\n",
        "\n",
        "print(best_cfg['learning_rate'])\n",
        "print(best_cfg['n_estimators'])\n",
        "print(best_acc)\n",
        "\n",
        "best_model = best_cfg['model']\n",
        "\n",
        "class AdaBoostTracker:\n",
        "    def __init__(self, n_estimators, learning_rate):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.estimator_errors = []\n",
        "        self.estimator_alphas = []\n",
        "        self.sample_weights_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]\n",
        "        w = np.ones(n) / n\n",
        "        y_signed = 2*y - 1\n",
        "        for i in range(self.n_estimators):\n",
        "            stump = DecisionTreeClassifier(max_depth=1, random_state=42+i)\n",
        "            stump.fit(X, y, sample_weight=w)\n",
        "            pred = stump.predict(X)\n",
        "            pred_signed = 2*pred - 1\n",
        "\n",
        "            incorrect = (pred != y)\n",
        "            err = np.sum(w[incorrect])\n",
        "            err = np.clip(err, 1e-10, 1-1e-10)\n",
        "\n",
        "            alpha = self.learning_rate * 0.5 * np.log((1-err)/err)\n",
        "\n",
        "            self.estimator_errors.append(err)\n",
        "            self.estimator_alphas.append(alpha)\n",
        "            self.sample_weights_history.append(w.copy())\n",
        "\n",
        "            w = w * np.exp(-alpha * y_signed * pred_signed)\n",
        "            w = w / np.sum(w)\n",
        "\n",
        "        self.final_weights = w\n",
        "        return self\n",
        "\n",
        "tracker = AdaBoostTracker(best_cfg['n_estimators'], best_cfg['learning_rate'])\n",
        "tracker.fit(X_train, y_train)\n",
        "\n",
        "print(tracker.estimator_errors)\n",
        "print(tracker.final_weights[:10])\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(range(1, len(tracker.estimator_errors)+1), tracker.estimator_errors, marker='o')\n",
        "plt.xlabel('iteration'); plt.ylabel('error'); plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(tracker.final_weights, bins=40)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "top_k = 10\n",
        "top_idx = np.argsort(tracker.final_weights)[-top_k:][::-1]\n",
        "print(top_idx)\n",
        "print(tracker.final_weights[top_idx])\n",
        "\n",
        "\n",
        "fi = best_model.feature_importances_\n",
        "names = X.columns if isinstance(X, pd.DataFrame) else df.drop('target', axis=1).columns\n",
        "order = np.argsort(fi)[::-1]\n",
        "top_k = min(10, len(fi))\n",
        "print(fi[order][:top_k])\n",
        "print(names[order][:top_k])\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(range(top_k), fi[order][:top_k][::-1])\n",
        "plt.yticks(range(top_k), names[order][:top_k][::-1])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "24yh8bel9Mbb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}